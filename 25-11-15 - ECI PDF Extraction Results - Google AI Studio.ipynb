{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a8c0e-20d4-456b-a65e-ca80e1da67a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### CODE 1 ####\n",
    "ECI constituency start/end mapper + row counter\n",
    "- Finds each \"Constituency <no> <name>\" (start)\n",
    "- Finds each \"TURN OUT\"/\"Turnout\" (end)\n",
    "- Pairs every TURN OUT with the most recent unmatched Constituency (stack logic)\n",
    "- Extracts number_of_rows = last serial number in the candidate list just before TURN OUT\n",
    "- Saves JSON with: constituency_no, constituency_name, start_page, end_page, number_of_rows\n",
    "Edit ELECTIONS_PDF and CONSTITUENCY_MAP_JSON and run.\n",
    "\n",
    "### CODE 2 ####\n",
    "Extraction of the data contained in the Election PDF \n",
    "- Creates PDF chunk of each page in the PDF\n",
    "- Sends them to Gemini ; saves them as JSON chunks\n",
    "- If constituency_no stops or not well extracted, delete the json chunk and re-run CODE 2\n",
    "- Code 2 is meant to be re-run and use past chunks\n",
    "\n",
    "### CODE 3 ####\n",
    "- Cleans the CSV file and rename columns to TCPD standards\n",
    "- creates N_Cand, n_rows, margin, margin_percentage, etc.\n",
    "\n",
    "### CODE 4 ####\n",
    "- Compare \"CONSTITUENCY_MAP_JSON\" and \"CLEANED_ELECTION_CSV\" on :\n",
    "    - number of rows\n",
    "    - page_number_start\n",
    "    - page_number_end\n",
    "    \n",
    "If one constituency has not been correctly exctracted\n",
    "    & delete its chunk file and re-run CODE 2\n",
    "\n",
    "\n",
    "# COST & EFFICIENCY\n",
    "- We tried to make this code as cost efficient as possible\n",
    "- For 372 pages, the Gemini cost us 17 euros (1700 Rs.)\n",
    "- This cost appears \"pretty expensive\"\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import pdfplumber  # pip install pdfplumber\n",
    "\n",
    "# --- Configure paths ---\n",
    "BASE_DIR = os.path.expanduser(\"~/Desktop/ECI Statistical Reports\")\n",
    "ELECTION_PDF_DIR = os.path.join(BASE_DIR, \"Statistical Reports PDF\")\n",
    "ELECTIONS_PDF = os.path.join(ELECTION_PDF_DIR, \"10-Detailed_Results_1744893339.pdf\")\n",
    "NO_OF_CONSTITUENCY = 288 # Number of constituencies in the state\n",
    "PDF_TOTAL_PAGES = 372  # Set to full run based on your input (all N pages will be processed if no chunk exist)\n",
    "\n",
    "# --- Configuration ---\n",
    "YOUR_API_KEY = \"YOUR_API_KEY\"  # ðŸ›‘ Replace with your actual key\n",
    "\n",
    "# Information about the State\n",
    "DELIMID = 4\n",
    "ELECTION_TYPE = \"State Assembly Election (AE)\" # \"Lok Sabha Election (GE)\"\n",
    "SABHA_TYPE = \"AE\" # AE or GE \n",
    "STATE_NAME = \"Maharashtra\"\n",
    "ASSEMBLY_NO = 14\n",
    "YEAR = 2024\n",
    "MONTH = 11\n",
    "POLL_NO = 0\n",
    "\n",
    "# Automatically Create Files name with State_Name + Year in their name.\n",
    "STATE_YEAR_PREFIX = f\"{STATE_NAME.replace(' ', '_')}_{SABHA_TYPE}_{YEAR}\"\n",
    "ELECTION_DIR = os.path.join(BASE_DIR, f\"{STATE_YEAR_PREFIX}\")\n",
    "CONSTITUENCY_MAP_JSON = os.path.join(ELECTION_DIR, f\"{STATE_YEAR_PREFIX}_constituency_start_end_map.json\")\n",
    "TEMP_CACHE_DIR = os.path.join(ELECTION_DIR, f\"{STATE_YEAR_PREFIX}_temp_json_cache_full_run\")\n",
    "ELECTION_JSON = os.path.join(ELECTION_DIR, f\"{STATE_YEAR_PREFIX}_elections.json\")\n",
    "ELECTION_CSV = os.path.join(ELECTION_DIR, f\"{STATE_YEAR_PREFIX}_elections.csv\")\n",
    "CLEANED_ELECTION_CSV = ELECTION_CSV #Change this in case you want an unclean and a cleaned final csv\n",
    "\n",
    "# ---- Silence pdfminer/pdfplumber chatter (e.g., \"CropBox missing...\") ----\n",
    "for name in (\"pdfminer\", \"pdfminer.layout\", \"pdfminer.pdfpage\", \"pdfplumber\"):\n",
    "    logging.getLogger(name).setLevel(logging.ERROR)\n",
    "    lg = logging.getLogger(name)\n",
    "    lg.propagate = False\n",
    "    if lg.handlers:\n",
    "        lg.handlers.clear()\n",
    "\n",
    "# ---- Regex patterns ----\n",
    "# Constituency header examples:\n",
    "#   \"Constituency 187 - COLABA ( TOTAL ELECTORS - 265326 )\"\n",
    "#   \"Constituency 188 PANVEL ( TOTAL ELECTORS - 652291 )\"\n",
    "CONST_RE = re.compile(\n",
    "    r'\\bConstituency\\s+(\\d+)\\s*(?:[-â€“â€”]\\s*)?([^\\n\\r(]+)?',\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "# TURN OUT appears as \"TURN OUT\" or \"Turnout\"\n",
    "TURNOUT_RE = re.compile(r'\\bTURN\\s*OUT\\b|\\bTURNOUT\\b', flags=re.IGNORECASE)\n",
    "\n",
    "# Candidate line number: line starting with integer + space (but NOT a header like \"1 - AKKALKUWA\")\n",
    "CAND_NUM_RE = re.compile(r'(?m)^\\s*(\\d{1,3})\\s+(?!-)')\n",
    "\n",
    "def clean_const_name(name_raw: Optional[str]) -> Optional[str]:\n",
    "    if not name_raw:\n",
    "        return None\n",
    "    name = name_raw.strip()\n",
    "    name = re.sub(r'\\s{2,}', ' ', name).strip(\" -\\t\\r\\n\")\n",
    "    name = re.sub(r'\\s*\\(.*$', '', name)                # drop \"( TOTAL ELECTORS - ... )\"\n",
    "    name = re.sub(r'\\s*TOTAL.*$', '', name, flags=re.IGNORECASE)\n",
    "    return name.strip() or None\n",
    "\n",
    "def extract_page_texts(pdf_path: str) -> List[str]:\n",
    "    \"\"\"Return list of page texts, 1-based index conceptually (we'll store in a 0-based list).\"\"\"\n",
    "    texts: List[str] = []\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            texts.append(page.extract_text() or \"\")\n",
    "    return texts\n",
    "\n",
    "def find_events(page_texts: List[str]) -> List[Tuple[int, int, str, Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Return list of events as tuples:\n",
    "      (page_index_1based, offset_in_page_text, 'start'|'turnout', payload)\n",
    "    payload for 'start': {'no': int, 'name': str|None}\n",
    "    payload for 'turnout': {}\n",
    "    \"\"\"\n",
    "    events: List[Tuple[int, int, str, Dict[str, Any]]] = []\n",
    "    for pidx_0, text in enumerate(page_texts):\n",
    "        pidx = pidx_0 + 1\n",
    "        for m in CONST_RE.finditer(text):\n",
    "            events.append((pidx, m.start(), \"start\", {\n",
    "                \"no\": int(m.group(1)),\n",
    "                \"name\": clean_const_name(m.group(2))\n",
    "            }))\n",
    "        for m in TURNOUT_RE.finditer(text):\n",
    "            events.append((pidx, m.start(), \"turnout\", {}))\n",
    "    events.sort(key=lambda e: (e[0], e[1]))\n",
    "    return events\n",
    "\n",
    "def pair_constituencies(events: List[Tuple[int, int, str, Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Pair each TURN OUT to the most recent unmatched Constituency.\n",
    "    Also keep start/end offsets so we can slice text precisely to count rows.\n",
    "    \"\"\"\n",
    "    mapping: Dict[int, Dict[str, Any]] = {}\n",
    "    open_stack: List[Tuple[int, int, int]] = []  # list of (const_no, start_page, start_offset)\n",
    "\n",
    "    for page, offset, evtype, payload in events:\n",
    "        if evtype == \"start\":\n",
    "            cno = int(payload[\"no\"])\n",
    "            cname = payload[\"name\"]\n",
    "            if cno not in mapping:\n",
    "                mapping[cno] = {\n",
    "                    \"constituency_no\": cno,\n",
    "                    \"constituency_name\": cname,\n",
    "                    \"page_number_start\": page,\n",
    "                    \"start_offset\": offset,\n",
    "                    \"page_number_end\": None,\n",
    "                    \"end_offset\": None,\n",
    "                    \"number_of_rows\": None\n",
    "                }\n",
    "            else:\n",
    "                # keep earliest start; backfill name if missing\n",
    "                if mapping[cno][\"constituency_name\"] is None and cname:\n",
    "                    mapping[cno][\"constituency_name\"] = cname\n",
    "                if page < mapping[cno][\"page_number_start\"]:\n",
    "                    mapping[cno][\"page_number_start\"] = page\n",
    "                    mapping[cno][\"start_offset\"] = offset\n",
    "            open_stack.append((cno, page, offset))\n",
    "\n",
    "        elif evtype == \"turnout\":\n",
    "            # Close the latest unmatched constituency\n",
    "            while open_stack:\n",
    "                cno, sp, so = open_stack.pop()\n",
    "                if mapping[cno][\"page_number_end\"] is None:\n",
    "                    mapping[cno][\"page_number_end\"] = page\n",
    "                    mapping[cno][\"end_offset\"] = offset\n",
    "                    break\n",
    "            # else: stray TURN OUT; ignore\n",
    "\n",
    "    # sorted list by constituency number\n",
    "    return sorted(mapping.values(), key=lambda d: d[\"constituency_no\"])\n",
    "\n",
    "def slice_constituency_text(entry: Dict[str, Any], page_texts: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Concatenate the text for a constituency between its start and TURN OUT markers (inclusive bounds trimmed).\n",
    "    \"\"\"\n",
    "    sp, so = entry[\"page_number_start\"], entry[\"start_offset\"]\n",
    "    ep, eo = entry[\"page_number_end\"], entry[\"end_offset\"]\n",
    "    if sp is None or ep is None:\n",
    "        return \"\"\n",
    "\n",
    "    # Convert to 0-based indexes for our list\n",
    "    sp0, ep0 = sp - 1, ep - 1\n",
    "\n",
    "    parts: List[str] = []\n",
    "    if sp == ep:\n",
    "        segment = page_texts[sp0][so:eo]\n",
    "        parts.append(segment)\n",
    "    else:\n",
    "        # first page: from start_offset to end of page\n",
    "        parts.append(page_texts[sp0][so:])\n",
    "        # middle pages: whole text\n",
    "        for p in range(sp0 + 1, ep0):\n",
    "            parts.append(page_texts[p])\n",
    "        # end page: from start of page to end_offset\n",
    "        parts.append(page_texts[ep0][:eo])\n",
    "\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def count_rows_in_segment(segment: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Return the largest leading integer on lines within the segment\n",
    "    (i.e., the last candidate serial number before TURN OUT).\n",
    "    \"\"\"\n",
    "    if not segment:\n",
    "        return None\n",
    "    nums = [int(m.group(1)) for m in CAND_NUM_RE.finditer(segment)]\n",
    "    return max(nums) if nums else None\n",
    "\n",
    "def build_mapping_with_rows(pdf_path: str) -> List[Dict[str, Any]]:\n",
    "    page_texts = extract_page_texts(pdf_path)\n",
    "    events = find_events(page_texts)\n",
    "    mapping = pair_constituencies(events)\n",
    "\n",
    "    # Sanity quick stats\n",
    "    total = len(mapping)\n",
    "    missing_end = sum(1 for m in mapping if m[\"page_number_end\"] is None)\n",
    "    print(f\"[Sanity] constituencies mapped: {total} | missing end: {missing_end}\")\n",
    "\n",
    "    # Compute number_of_rows for each completed constituency\n",
    "    for m in mapping:\n",
    "        if m[\"page_number_end\"] is not None and m[\"page_number_start\"] is not None:\n",
    "            seg = slice_constituency_text(m, page_texts)\n",
    "            m[\"number_of_rows\"] = count_rows_in_segment(seg)\n",
    "\n",
    "    # Remove offsets from final output (keep page_number_start/page_number_end + number_of_rows)\n",
    "    for m in mapping:\n",
    "        m.pop(\"start_offset\", None)\n",
    "        m.pop(\"end_offset\", None)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "def main():\n",
    "    mapping = build_mapping_with_rows(ELECTIONS_PDF)\n",
    "\n",
    "    # âœ… Ensure output directory exists\n",
    "    os.makedirs(ELECTION_DIR, exist_ok=True)\n",
    "\n",
    "    Path(CONSTITUENCY_MAP_JSON).write_text(json.dumps(mapping, indent=2, ensure_ascii=False), encoding=\"utf-8\")\n",
    "    print(f\"Saved mapping â†’ {CONSTITUENCY_MAP_JSON}\")\n",
    "\n",
    "    # Print requested constituency\n",
    "    tgt = next((m for m in mapping if m[\"constituency_no\"] == NO_OF_CONSTITUENCY), None)\n",
    "    if tgt:\n",
    "        print(f\"\\nConstituency {tgt['constituency_no']} â€” {tgt['constituency_name'] or '(name unknown)'}\")\n",
    "        print(f\"  page_number_start: {tgt['page_number_start']}\")\n",
    "        print(f\"  page_number_end:   {tgt['page_number_end']}\")\n",
    "        print(f\"  number_of_rows:    {tgt['number_of_rows']}\")\n",
    "    else:\n",
    "        print(f\"\\nConstituency {NO_OF_CONSTITUENCY} not found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc1a13-b468-46b9-9fe9-95ac47557309",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE 2 ####\n",
    "# - Extract PDF chunks of each Page \n",
    "# - Build chunks json into a final JSON & CSV file for clean ECI results\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pypdf\n",
    "import google.generativeai as genai\n",
    "from google.api_core import exceptions\n",
    "import time\n",
    "import shutil\n",
    "import re  # Import regex for splitting constituency name\n",
    "\n",
    "# --- Parameters ---\n",
    "CHUNK_SIZE = 1                            # 1 page per chunk (Most reliable)\n",
    "MODEL_NAME = 'models/gemini-2.5-pro'     # The reliable model\n",
    "\n",
    "# --- JSON Schema (Kept simple for extraction, transformation happens in Python) ---\n",
    "JSON_SCHEMA = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"page_number\": \"integer (The page number of the PDF this data was extracted from)\",\n",
    "    \"constituency_name\": \"string (e.g., '1 - AKKALKUWA (ST)')\",\n",
    "    \"total_electors\": \"integer or null (Extract from 'TOTAL ELECTORS' in the constituency header)\",\n",
    "    \"turnout_total_votes\": \"integer or null (Extract the 'TOTAL' vote count from the 'TURN OUT' row)\",\n",
    "    \"turnout_percentage\": \"float or null (Extract the '% VOTES POLLED' from the 'TURN OUT' row)\",\n",
    "    \"page_number_end\": \"integer or null (the page where the constituency ends; null if it continues)\",\n",
    "    \"candidates\": [\n",
    "      {\n",
    "        \"rank\": \"integer\",\n",
    "        \"name\": \"string (MUST include NOTA as a candidate named 'NOTA')\",\n",
    "        \"gender\": \"string (MALE/FEMALE/NULL for NOTA)\",\n",
    "        \"age\": \"integer or null\",\n",
    "        \"category\": \"string or null\",\n",
    "        \"party\": \"string (e.g., SHS or NOTA)\",\n",
    "        \"symbol\": \"string\",\n",
    "        \"general_votes\": \"integer\",\n",
    "        \"postal_votes\": \"integer\",\n",
    "        \"total_votes\": \"integer\",\n",
    "        \"percent_valid_votes\": \"float\",\n",
    "        \"percent_total_electors\": \"float\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# --- Core Functions ---\n",
    "\n",
    "def get_pdf_page_count(input_path):\n",
    "    \"\"\"Safely determines the total number of pages in the PDF.\"\"\"\n",
    "    try:\n",
    "        reader = pypdf.PdfReader(input_path)\n",
    "        return len(reader.pages)\n",
    "    except Exception:\n",
    "        print(f\"ERROR: The input PDF '{input_path}' was not found or is corrupted.\")\n",
    "        return 0\n",
    "\n",
    "def create_pdf_chunk(input_path, output_path, start_page, end_page):\n",
    "    \"\"\"Creates a new PDF with only the specified page range (1-indexed).\"\"\"\n",
    "    reader = pypdf.PdfReader(input_path)\n",
    "    writer = pypdf.PdfWriter()\n",
    "\n",
    "    for page_num in range(start_page - 1, end_page):\n",
    "        writer.add_page(reader.pages[page_num])\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        writer.write(f)\n",
    "\n",
    "def clean_json_response(raw_response):\n",
    "    \"\"\"Cleans the AI's response to extract the JSON list.\"\"\"\n",
    "    try:\n",
    "        start_index = raw_response.find('[')\n",
    "        end_index = raw_response.rfind(']')\n",
    "        if start_index != -1 and end_index != -1 and end_index > start_index:\n",
    "            json_str = raw_response[start_index: end_index + 1]\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            return []\n",
    "    except json.JSONDecodeError:\n",
    "        return []\n",
    "\n",
    "def extract_data_from_chunk(pdf_chunk_path, last_constituency_context, page_num):\n",
    "    \"\"\"Uploads a PDF chunk and extracts data using the specified Gemini model.\"\"\"\n",
    "    print(f\"\\n--- Processing Chunk for Page {page_num}: {pdf_chunk_path} ---\")\n",
    "    if last_constituency_context:\n",
    "        print(f\"--- Context: Continuing constituency '{last_constituency_context}' ---\")\n",
    "\n",
    "    model = genai.GenerativeModel(MODEL_NAME)\n",
    "    max_retries = 3\n",
    "    backoff_seconds = 5\n",
    "\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        uploaded_file = None\n",
    "        try:\n",
    "            uploaded_file = genai.upload_file(path=pdf_chunk_path, display_name=pdf_chunk_path)\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            You are a data extraction bot. Extract all election data from the provided PDF chunk (page {page_num} of the full PDF).\n",
    "            Your output MUST be a valid JSON list adhering to this exact schema:\n",
    "            {JSON_SCHEMA}\n",
    "\n",
    "            INSTRUCTIONS:\n",
    "            1. For every constituency detected in this chunk, you MUST include a 'page_number' field with value {page_num}.\n",
    "            2. Constituency data can span multiple pages. If this page continues a constituency from the previous page, return all extracted candidates/data for it.\n",
    "            3. The constituency boundary is marked by the 'Constituency' header (e.g. 'Constituency 1 - AKKALKUWA (ST) ( TOTAL ELECTORS - ... )') and ends immediately after the 'TURN OUT TOTAL:' row for that constituency.\n",
    "            4. You MUST extract 'constituency_name' exactly as it appears in the header, but WITHOUT the word 'Constituency'. For example, from 'Constituency 1 - AKKALKUWA (ST) ( TOTAL ELECTORS - 319481)' you must use:\n",
    "               \"constituency_name\": \"1 - AKKALKUWA (ST)\".\n",
    "            5. You MUST extract:\n",
    "               - 'total_electors' from the '( TOTAL ELECTORS - ... )' part of the header,\n",
    "               - 'turnout_total_votes' (the TOTAL column) and\n",
    "               - 'turnout_percentage' (the '% VOTES POLLED' value)\n",
    "               from the 'TURN OUT TOTAL:' row whenever that row is present on this page.\n",
    "            6. You MUST treat **NOTA** as a regular candidate and include it in the 'candidates' list.\n",
    "            7. For each constituency, you MUST extract all candidate rows in correct order, up to (and not beyond) the 'TURN OUT TOTAL:' row for that constituency. The extraction for a given constituency is only complete once the 'TURN OUT TOTAL:' row has been seen.\n",
    "            8. You MUST include a 'page_number_end' field for each constituency object:\n",
    "               - If this page contains the 'TURN OUT TOTAL:' row for that constituency, set 'page_number_end' to {page_num}.\n",
    "               - If the 'TURN OUT TOTAL:' row is NOT present for that constituency in this chunk (i.e. the candidate list continues on the next page), set 'page_number_end' to null for that constituency in this page.\n",
    "            9. If this page starts in the middle of a constituency (continuation from a previous page), do NOT repeat candidates that already appeared on previous pages. Only add the remaining candidate rows visible on this page, followed by the 'TURN OUT TOTAL:' row if it appears here.\n",
    "            10. Your final answer MUST be a single valid JSON list matching the schema exactly.\n",
    "            \"\"\"\n",
    "\n",
    "            if last_constituency_context:\n",
    "                prompt += f\"\"\"\n",
    "            CONTEXT: The previous page ended during the candidate list for constituency '{last_constituency_context}'.\n",
    "            Any data at the very beginning of this new chunk belongs to '{last_constituency_context}' until a new 'Constituency' header appears.\n",
    "            \"\"\"\n",
    "\n",
    "            response = model.generate_content([prompt, uploaded_file])\n",
    "\n",
    "            # Cleanup upload\n",
    "            try:\n",
    "                genai.delete_file(uploaded_file.name)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            extracted_data = clean_json_response(response.text)\n",
    "\n",
    "            new_context = None\n",
    "            if extracted_data:\n",
    "                last_entry = extracted_data[-1]\n",
    "                # If the last constituency on this page has no TURN OUT data yet or\n",
    "                # explicit page_number_end is null, we assume it continues.\n",
    "                if (last_entry.get(\"turnout_total_votes\") is None or\n",
    "                        last_entry.get(\"page_number_end\") is None):\n",
    "                    new_context = last_entry.get(\"constituency_name\")\n",
    "                    if new_context:\n",
    "                        print(f\"INFO: Page {page_num} ends during constituency: {new_context}\")\n",
    "\n",
    "            return extracted_data, new_context\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR: API call failed for page {page_num}, attempt {attempt}/{max_retries}: {e}\")\n",
    "            # Cleanup upload if something went wrong\n",
    "            if uploaded_file is not None:\n",
    "                try:\n",
    "                    genai.delete_file(uploaded_file.name)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if attempt < max_retries:\n",
    "                print(f\"Retrying page {page_num} after {backoff_seconds} seconds...\")\n",
    "                time.sleep(backoff_seconds)\n",
    "            else:\n",
    "                print(f\"Giving up on page {page_num} after {max_retries} failed attempts.\")\n",
    "                # Return empty data but keep last_context so the loop can continue\n",
    "                return [], last_constituency_context\n",
    "\n",
    "# --- Constituency Splitting Logic ---\n",
    "\n",
    "def split_constituency_name(full_name):\n",
    "    \"\"\"\n",
    "    Splits a constituency name string (e.g., '1 - AKKALKUWA (ST)') into three components.\n",
    "    \"\"\"\n",
    "    constituency_no = None\n",
    "    constituency_name_only = full_name\n",
    "    constituency_type = \"GENERAL\"\n",
    "\n",
    "    match = re.match(r'(\\d+)\\s*-\\s*(.*?)\\s*(\\((ST|SC)\\))?$', full_name.strip())\n",
    "\n",
    "    if match:\n",
    "        constituency_no = int(match.group(1))\n",
    "\n",
    "        # Capture constituency name without (ST/SC)\n",
    "        name_part = match.group(2).strip()\n",
    "        constituency_name_only = name_part.replace(' (ST)', '').replace(' (SC)', '').strip()\n",
    "\n",
    "        # Check for type\n",
    "        type_part = match.group(4)\n",
    "        if type_part in ['ST', 'SC']:\n",
    "            constituency_type = type_part\n",
    "\n",
    "    return constituency_no, constituency_name_only, constituency_type\n",
    "\n",
    "# --- Consolidation Function ---\n",
    "\n",
    "def consolidate_data(raw_data):\n",
    "    \"\"\"\n",
    "    Consolidates fragmented data and adds new split constituency fields,\n",
    "    including page_number_start and page_number_end.\n",
    "    \"\"\"\n",
    "    if not raw_data:\n",
    "        return []\n",
    "\n",
    "    # Pass 1: Collect all available metadata and candidates grouped by constituency name\n",
    "    constituency_map = {}\n",
    "\n",
    "    for item in raw_data:\n",
    "        name = item.get(\"constituency_name\")\n",
    "        if not name:\n",
    "            continue\n",
    "\n",
    "        if name not in constituency_map:\n",
    "            constituency_map[name] = {\n",
    "                \"records\": [],\n",
    "                \"total_electors\": None,\n",
    "                \"turnout_total_votes\": None,\n",
    "                \"turnout_percentage\": None,\n",
    "                \"page_number_start\": None,\n",
    "                \"page_number_end\": None,\n",
    "            }\n",
    "\n",
    "        group = constituency_map[name]\n",
    "        group[\"records\"].append(item)\n",
    "\n",
    "        # Collect the best data for fixed fields\n",
    "        if item.get(\"total_electors\") is not None:\n",
    "            group[\"total_electors\"] = item[\"total_electors\"]\n",
    "        if item.get(\"turnout_total_votes\") is not None:\n",
    "            group[\"turnout_total_votes\"] = item[\"turnout_total_votes\"]\n",
    "        if item.get(\"turnout_percentage\") is not None:\n",
    "            group[\"turnout_percentage\"] = item[\"turnout_percentage\"]\n",
    "\n",
    "        # Track start page (minimum page_number)\n",
    "        pn = item.get(\"page_number\")\n",
    "        if pn is not None:\n",
    "            if group[\"page_number_start\"] is None or pn < group[\"page_number_start\"]:\n",
    "                group[\"page_number_start\"] = pn\n",
    "\n",
    "        # Track end page (maximum non-null page_number_end)\n",
    "        pne = item.get(\"page_number_end\")\n",
    "        if pne is not None:\n",
    "            if group[\"page_number_end\"] is None or pne > group[\"page_number_end\"]:\n",
    "                group[\"page_number_end\"] = pne\n",
    "\n",
    "    # Pass 2: Re-assemble the final consolidated list and apply constituency split\n",
    "    final_data = []\n",
    "\n",
    "    # Sort constituencies by the page they first appear on\n",
    "    def _start_key(item):\n",
    "        data = item[1]\n",
    "        if data[\"page_number_start\"] is not None:\n",
    "            return data[\"page_number_start\"]\n",
    "        # Fallback if for some reason page_number_start is missing\n",
    "        return data[\"records\"][0].get(\"page_number\", 0)\n",
    "\n",
    "    sorted_constituencies = sorted(constituency_map.items(), key=_start_key)\n",
    "\n",
    "    for name, data in sorted_constituencies:\n",
    "        all_candidates = []\n",
    "        for record in data[\"records\"]:\n",
    "            all_candidates.extend(record.get(\"candidates\", []))\n",
    "\n",
    "        # Apply the splitting logic (number / name / type)\n",
    "        constituency_no, constituency_name_only, constituency_type = split_constituency_name(name)\n",
    "\n",
    "        # Fallbacks if start/end pages somehow missing\n",
    "        page_start = data[\"page_number_start\"] or data[\"records\"][0].get(\"page_number\")\n",
    "        page_end = data[\"page_number_end\"] or data[\"records\"][-1].get(\"page_number\")\n",
    "\n",
    "        # Create one consolidated record\n",
    "        consolidated_record = {\n",
    "            \"page_number_start\": page_start,\n",
    "            \"page_number_end\": page_end,\n",
    "            \"constituency_full_name\": name,       # Original for reference\n",
    "            \"constituency_no\": constituency_no,\n",
    "            \"constituency_name\": constituency_name_only,  # Cleaned name\n",
    "            \"constituency_type\": constituency_type,\n",
    "            \"total_electors\": data[\"total_electors\"],\n",
    "            \"turnout_total_votes\": data[\"turnout_total_votes\"],\n",
    "            \"turnout_percentage\": data[\"turnout_percentage\"],\n",
    "            \"candidates\": all_candidates,\n",
    "        }\n",
    "\n",
    "        final_data.append(consolidated_record)\n",
    "\n",
    "    return final_data\n",
    "\n",
    "# --- CSV Saving Function ---\n",
    "def save_csv(data, filename):\n",
    "    \"\"\"\n",
    "    Flattens the consolidated JSON data, applies renames, and saves it to a CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define NEW fieldnames with updated names and order\n",
    "    fieldnames = [\n",
    "        'constituency_no',\n",
    "        'constituency_name',\n",
    "        'constituency_type',\n",
    "        'total_electors',\n",
    "        'turnout_total_votes',\n",
    "        'turnout_percentage',\n",
    "        'position',        # Renamed from 'rank'\n",
    "        'candidate',       # Renamed from 'name'\n",
    "        'gender',\n",
    "        'age',\n",
    "        'candidate_type',  # Renamed from 'category'\n",
    "        'party',\n",
    "        'symbol',\n",
    "        'general_votes',\n",
    "        'postal_votes',\n",
    "        'total_votes',\n",
    "        'percent_valid_votes',\n",
    "        'percent_total_electors',\n",
    "        'constituency_full_name',  # Original for debug/reference\n",
    "        'page_number_start',\n",
    "        'page_number_end'          # âœ… NEW\n",
    "    ]\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for constituency in data:\n",
    "        base_data = {\n",
    "            'constituency_no': constituency.get('constituency_no'),\n",
    "            'constituency_name': constituency.get('constituency_name'),\n",
    "            'constituency_type': constituency.get('constituency_type'),\n",
    "            'constituency_full_name': constituency.get('constituency_full_name'),\n",
    "            'page_number_start': constituency.get('page_number_start'),\n",
    "            'page_number_end': constituency.get('page_number_end'),  # âœ… NEW\n",
    "            'total_electors': constituency.get('total_electors'),\n",
    "            'turnout_total_votes': constituency.get('turnout_total_votes'),\n",
    "            'turnout_percentage': constituency.get('turnout_percentage')\n",
    "        }\n",
    "\n",
    "        for candidate in constituency.get('candidates', []):\n",
    "            transformed_candidate = {\n",
    "                'position': candidate.get('rank'),\n",
    "                'candidate': candidate.get('name'),\n",
    "                'gender': candidate.get('gender'),\n",
    "                'age': candidate.get('age'),\n",
    "                'candidate_type': candidate.get('category'),\n",
    "                'party': candidate.get('party'),\n",
    "                'symbol': candidate.get('symbol'),\n",
    "                'general_votes': candidate.get('general_votes'),\n",
    "                'postal_votes': candidate.get('postal_votes'),\n",
    "                'total_votes': candidate.get('total_votes'),\n",
    "                'percent_valid_votes': candidate.get('percent_valid_votes'),\n",
    "                'percent_total_electors': candidate.get('percent_total_electors')\n",
    "            }\n",
    "            row = {**base_data, **transformed_candidate}\n",
    "            rows.append(row)\n",
    "\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"âœ… Data successfully flattened, transformed, and saved to CSV file: {filename}\")\n",
    "\n",
    "# --- Resume Helpers ---\n",
    "\n",
    "def get_last_context_before_page(start_page):\n",
    "    \"\"\"\n",
    "    For a missing segment starting at start_page, look at the previous page\n",
    "    (start_page - 1) in the cache and decide if we are in the middle of a constituency.\n",
    "    Returns the constituency_name if the previous page ended mid-constituency, else None.\n",
    "    \"\"\"\n",
    "    prev_page = start_page - 1\n",
    "    if prev_page < 1:\n",
    "        return None\n",
    "\n",
    "    cache_filename = os.path.join(TEMP_CACHE_DIR, f\"chunk_{prev_page}_data.json\")\n",
    "    if not os.path.exists(cache_filename):\n",
    "        print(f\"WARNING: No cache file found for previous page {prev_page}. No context.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with open(cache_filename, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not read cache file for page {prev_page}: {e}\")\n",
    "        return None\n",
    "\n",
    "    if not data:\n",
    "        return None\n",
    "\n",
    "    last_entry = data[-1]\n",
    "    cname = last_entry.get(\"constituency_name\")\n",
    "\n",
    "    # Continuation if turnout_total_votes is None OR page_number_end is None\n",
    "    if cname and (last_entry.get(\"turnout_total_votes\") is None or\n",
    "                  last_entry.get(\"page_number_end\") is None):\n",
    "        print(f\"Resumed context for page {start_page} from cached page {prev_page}: '{cname}'\")\n",
    "        return cname\n",
    "\n",
    "    return None\n",
    "\n",
    "def detect_resume_state(pages_to_process):\n",
    "    \"\"\"\n",
    "    Determine which pages are missing from cache and need to be (re-)extracted.\n",
    "\n",
    "    Returns:\n",
    "        missing_segments: list of (start_page, end_page) tuples (inclusive)\n",
    "        segment_contexts: dict {start_page: last_constituency_context_before_segment}\n",
    "    \"\"\"\n",
    "    if not os.path.exists(TEMP_CACHE_DIR):\n",
    "        os.makedirs(TEMP_CACHE_DIR)\n",
    "        print(\"No existing cache. All pages considered missing.\")\n",
    "        # Entire range is missing; no context before page 1\n",
    "        return [(1, pages_to_process)], {1: None}\n",
    "\n",
    "    cache_files = [f for f in os.listdir(TEMP_CACHE_DIR)\n",
    "                   if f.startswith(\"chunk_\") and f.endswith(\"_data.json\")]\n",
    "    if not cache_files:\n",
    "        print(\"Cache directory is empty. All pages considered missing.\")\n",
    "        return [(1, pages_to_process)], {1: None}\n",
    "\n",
    "    # Extract page numbers from file names like \"chunk_60_data.json\"\n",
    "    try:\n",
    "        processed_pages = sorted(int(f.split('_')[1]) for f in cache_files)\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Could not parse page numbers from cache files: {e}\")\n",
    "        print(\"Assuming cache unusable. All pages considered missing.\")\n",
    "        return [(1, pages_to_process)], {1: None}\n",
    "\n",
    "    pages_set = set(processed_pages)\n",
    "\n",
    "    # Find missing pages in range 1..pages_to_process\n",
    "    missing_pages = [p for p in range(1, pages_to_process + 1) if p not in pages_set]\n",
    "\n",
    "    if not missing_pages:\n",
    "        print(f\"All {pages_to_process} pages already present in cache (1..{pages_to_process}). No extraction needed.\")\n",
    "        return [], {}\n",
    "\n",
    "    # Group missing pages into contiguous segments: e.g. [1,2,3, 10,11] -> [(1,3), (10,11)]\n",
    "    missing_segments = []\n",
    "    seg_start = missing_pages[0]\n",
    "    prev_p = seg_start\n",
    "\n",
    "    for p in missing_pages[1:]:\n",
    "        if p == prev_p + 1:\n",
    "            prev_p = p\n",
    "        else:\n",
    "            missing_segments.append((seg_start, prev_p))\n",
    "            seg_start = p\n",
    "            prev_p = p\n",
    "    missing_segments.append((seg_start, prev_p))\n",
    "\n",
    "    # For each segment, compute context from previous page\n",
    "    segment_contexts = {}\n",
    "    for start_page, end_page in missing_segments:\n",
    "        ctx = get_last_context_before_page(start_page)\n",
    "        segment_contexts[start_page] = ctx\n",
    "\n",
    "    print(f\"Missing page segments to (re-)extract: {missing_segments}\")\n",
    "    return missing_segments, segment_contexts\n",
    "\n",
    "# --- Aggregation Helper ---\n",
    "\n",
    "def load_and_consolidate_from_cache():\n",
    "    \"\"\"Load all chunk JSON files from cache, consolidate, and return final data.\"\"\"\n",
    "    raw_aggregated_data = []\n",
    "    if not os.path.exists(TEMP_CACHE_DIR):\n",
    "        print(\"No cache directory found. Nothing to aggregate.\")\n",
    "        return []\n",
    "\n",
    "    cache_files = [f for f in os.listdir(TEMP_CACHE_DIR) if f.endswith(\".json\")]\n",
    "    if not cache_files:\n",
    "        print(\"Cache directory is empty. Nothing to aggregate.\")\n",
    "        return []\n",
    "\n",
    "    # Aggregate raw data from disk cache\n",
    "    for filename in sorted(cache_files, key=lambda x: int(x.split('_')[1])):\n",
    "        full_path = os.path.join(TEMP_CACHE_DIR, filename)\n",
    "        try:\n",
    "            with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                raw_aggregated_data.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: Could not load data from {filename}: {e}\")\n",
    "\n",
    "    # Consolidate and Transform the data\n",
    "    final_consolidated_data = consolidate_data(raw_aggregated_data)\n",
    "    return final_consolidated_data\n",
    "\n",
    "# --- Main Execution Loop with Resumable Caching ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Runs the full extraction with 1-page chunks and disk caching, resumable.\"\"\"\n",
    "\n",
    "    # 1. Initialization and Setup\n",
    "    try:\n",
    "        genai.configure(api_key=YOUR_API_KEY)\n",
    "    except Exception:\n",
    "        print(\"ERROR: Could not configure Google GenAI client. Please ensure the API key and library are correct.\")\n",
    "        return\n",
    "\n",
    "    total_pdf_pages = get_pdf_page_count(ELECTIONS_PDF)\n",
    "    if total_pdf_pages == 0:\n",
    "        return\n",
    "\n",
    "    # Use the full page count specified\n",
    "    pages_to_process = min(PDF_TOTAL_PAGES, total_pdf_pages)\n",
    "\n",
    "    # Detect which pages are missing and need (re-)extraction\n",
    "    missing_segments, segment_contexts = detect_resume_state(pages_to_process)\n",
    "\n",
    "    temp_pdf_files = []\n",
    "\n",
    "    if not missing_segments:\n",
    "        print(f\"--- No pages left to extract (all 1..{pages_to_process} already cached). Skipping extraction. ---\")\n",
    "    else:\n",
    "        print(f\"--- Will (re-)extract only missing pages, segments: {missing_segments} ---\")\n",
    "\n",
    "    # 2. Extraction Loop (only missing pages)\n",
    "    try:\n",
    "        for (start_page, end_page) in missing_segments:\n",
    "            last_context = segment_contexts.get(start_page)\n",
    "            print(f\"\\n=== Processing missing segment {start_page}â€“{end_page}, initial context: {last_context} ===\")\n",
    "\n",
    "            for page_num in range(start_page, end_page + 1):\n",
    "                chunk_filename = f\"temp_chunk_{page_num}.pdf\"\n",
    "                temp_pdf_files.append(chunk_filename)\n",
    "\n",
    "                # Create 1-page chunk\n",
    "                create_pdf_chunk(ELECTIONS_PDF, chunk_filename, page_num, page_num)\n",
    "\n",
    "                # Extract Data\n",
    "                chunk_data, last_context = extract_data_from_chunk(chunk_filename, last_context, page_num)\n",
    "\n",
    "                # Save to Disk Cache (raw, fragmented data)\n",
    "                if chunk_data:\n",
    "                    cache_filename = os.path.join(TEMP_CACHE_DIR, f\"chunk_{page_num}_data.json\")\n",
    "                    # Ensure page_number is set correctly\n",
    "                    for item in chunk_data:\n",
    "                        item['page_number'] = page_num\n",
    "                    with open(cache_filename, 'w', encoding='utf-8') as f:\n",
    "                        json.dump(chunk_data, f, indent=2)\n",
    "                    print(f\"INFO: Data for page {page_num} saved to disk cache.\")\n",
    "\n",
    "                # Small throttle to avoid overwhelming the API / network\n",
    "                time.sleep(0.5)\n",
    "                print(\"-\" * 20)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nFATAL ERROR during processing loop: {e}\")\n",
    "\n",
    "    # --- Final Steps: Aggregation and Saving ---\n",
    "    print(\"\\n--- Aggregating, Consolidating, and Transforming Data from Cache ---\")\n",
    "\n",
    "    final_consolidated_data = load_and_consolidate_from_cache()\n",
    "\n",
    "    if final_consolidated_data:\n",
    "        # Save final aggregated JSON\n",
    "        with open(ELECTION_JSON, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_consolidated_data, f, indent=2)\n",
    "        print(f\"âœ… Final consolidated data saved to JSON: {ELECTION_JSON}\")\n",
    "\n",
    "        # Save the final transformed CSV\n",
    "        save_csv(final_consolidated_data, ELECTION_CSV)\n",
    "    else:\n",
    "        print(\"WARNING: No data was extracted or consolidated.\")\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"\\nCleaning up temporary PDF chunk files created in THIS run...\")\n",
    "    for filename in temp_pdf_files:\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "\n",
    "    # IMPORTANT: We keep TEMP_CACHE_DIR to allow future resumes / re-aggregation.\n",
    "    print(f\"INFO: Keeping data cache in {TEMP_CACHE_DIR} for potential resume/debug.\")\n",
    "    print(\"Full extraction pipeline (resumable) finalized.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19880a6c-b977-4b33-82e6-0b87bf5372a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE 3 ####\n",
    "# - Clean the csv to TCPD column names standards\n",
    "# Create N_Cand, margin, margin_percentage\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# =====================================================\n",
    "# 0. LOAD CSV\n",
    "# =====================================================\n",
    "print(\"Step 1: Loading existing CSV...\")\n",
    "df = pd.read_csv(ELECTION_CSV)\n",
    "\n",
    "# =====================================================\n",
    "# 1. METADATA COLUMNS (SAME VALUE FOR ALL ROWS)\n",
    "# =====================================================\n",
    "print(\"Step 2: Updating metadata columns...\")\n",
    "\n",
    "df[\"delimid\"] = DELIMID\n",
    "df[\"election_type\"] = ELECTION_TYPE\n",
    "df[\"state_name\"] = STATE_NAME\n",
    "df[\"assembly_no\"] = ASSEMBLY_NO\n",
    "df[\"year\"] = YEAR\n",
    "df[\"month\"] = MONTH\n",
    "df[\"poll_no\"] = POLL_NO\n",
    "\n",
    "# =====================================================\n",
    "# 2. SIMPLE RENAMES (DO NOT DROP ORIGINAL COLUMNS)\n",
    "# =====================================================\n",
    "print(\"Step 3: Simple renaming / copies...\")\n",
    "\n",
    "rename_map = {\n",
    "    \"total_electors\": \"electors\",\n",
    "    \"turnout_total_votes\": \"valid_votes\",\n",
    "    \"candidate_name\": \"candidate\",\n",
    "    \"gender\": \"sex\",\n",
    "    \"total_votes\": \"votes\",\n",
    "    \"percent_valid_votes\": \"vote_share_percentage\",\n",
    "    \"percent_total_electors\": \"electors_share_percentage\"\n",
    "}\n",
    "\n",
    "# --- create a case-insensitive lookup for the current df columns ---\n",
    "lower_cols = {c.lower(): c for c in df.columns}\n",
    "\n",
    "# --- STEP 1: if both src and tgt exist (case-insensitive), merge values then drop src ---\n",
    "for src_lower, tgt in rename_map.items():\n",
    "    if src_lower in lower_cols and tgt.lower() in lower_cols:\n",
    "        src_col = lower_cols[src_lower]\n",
    "        tgt_col = lower_cols[tgt.lower()]\n",
    "        df[tgt_col] = df[tgt_col].fillna(df[src_col])\n",
    "        df.drop(columns=[src_col], inplace=True)\n",
    "        # remove from map since it's already handled\n",
    "        lower_cols.pop(src_lower, None)\n",
    "\n",
    "# rebuild lookup because df.columns changed\n",
    "lower_cols = {c.lower(): c for c in df.columns}\n",
    "\n",
    "# --- STEP 2: rename remaining src â†’ tgt (case-insensitive) ---\n",
    "rename_dict = {}\n",
    "for src_lower, tgt in rename_map.items():\n",
    "    if src_lower in lower_cols:\n",
    "        rename_dict[lower_cols[src_lower]] = tgt\n",
    "\n",
    "df.rename(columns=rename_dict, inplace=True)\n",
    "\n",
    "# =====================================================\n",
    "# 3. CLEAN NUMERIC COLUMNS\n",
    "# =====================================================\n",
    "print(\"Step 4: Cleaning numeric columns...\")\n",
    "\n",
    "numeric_cols = [\"votes\", \"valid_votes\", \"age\", \"position\", \"constituency_no\", \"assembly_no\"]\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .astype(str)\n",
    "            .str.replace(\",\", \"\", regex=False)\n",
    "            .str.strip()\n",
    "        )\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. N_Cand (PER CONST, EXCLUDING NOTA)\n",
    "# =====================================================\n",
    "print(\"Step 5: Calculating N_Cand (excluding NOTA)...\")\n",
    "\n",
    "# Group at constituency level only\n",
    "group_keys_ncand = [\"constituency_no\"]\n",
    "group_keys_ncand = [c for c in group_keys_ncand if c in df.columns]\n",
    "\n",
    "if group_keys_ncand:\n",
    "    # Build NOTA mask\n",
    "    nota_mask = pd.Series(False, index=df.index)\n",
    "\n",
    "    if \"candidate\" in df.columns:\n",
    "        nota_mask |= (\n",
    "            df[\"candidate\"]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.upper()\n",
    "            .eq(\"NOTA\")\n",
    "        )\n",
    "\n",
    "    if \"party\" in df.columns:\n",
    "        nota_mask |= (\n",
    "            df[\"party\"]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.upper()\n",
    "            .eq(\"NOTA\")\n",
    "        )\n",
    "\n",
    "    # Keep only real candidates (exclude NOTA)\n",
    "    df_real = df[~nota_mask].copy()\n",
    "\n",
    "    # Count real candidates per constituency\n",
    "    ncand_counts = (\n",
    "        df_real\n",
    "        .groupby(group_keys_ncand)\n",
    "        .size()\n",
    "        .reset_index(name=\"n_cand\")\n",
    "    )\n",
    "\n",
    "    # Merge counts back into full dataframe (NOTA rows included but not counted)\n",
    "    df = df.merge(ncand_counts, on=group_keys_ncand, how=\"left\")\n",
    "\n",
    "    # Fill missing with 0\n",
    "    df[\"n_cand\"] = df[\"n_cand\"].fillna(0).astype(int)\n",
    "\n",
    "else:\n",
    "    # If grouping keys missing, create N_Cand=0 column\n",
    "    if \"n_cand\" not in df.columns:\n",
    "        df[\"n_cand\"] = 0\n",
    "\n",
    "group_keys_nrows = [\"constituency_no\"]\n",
    "group_keys_nrows = [c for c in group_keys_nrows if c in df.columns]\n",
    "\n",
    "if group_keys_nrows:\n",
    "    # Count ALL rows per constituency (including NOTA)\n",
    "    nrows_counts = (\n",
    "        df\n",
    "        .groupby(group_keys_nrows)\n",
    "        .size()\n",
    "        .reset_index(name=\"number_of_rows\")\n",
    "    )\n",
    "\n",
    "    df = df.merge(nrows_counts, on=group_keys_nrows, how=\"left\")\n",
    "    df[\"number_of_rows\"] = df[\"number_of_rows\"].fillna(0).astype(int)\n",
    "    \n",
    "# =====================================================\n",
    "# 5. CONSTITUENCY TYPE\n",
    "# =====================================================\n",
    "print(\"Step 6: Cleaning Constituency_Type...\")\n",
    "\n",
    "if \"constituency_type\" in df.columns:\n",
    "    df[\"constituency_type\"] = df[\"constituency_type\"].replace(\"GENERAL\", \"GEN\")\n",
    "\n",
    "# =====================================================\n",
    "# 6. DEPOSIT LOST\n",
    "# =====================================================\n",
    "print(\"Step 7: Calculating deposit_lost...\")\n",
    "\n",
    "if \"votes\" in df.columns and \"valid_votes\" in df.columns:\n",
    "    df[\"votes\"] = pd.to_numeric(df[\"votes\"], errors=\"coerce\")\n",
    "    df[\"valid_votes\"] = pd.to_numeric(df[\"valid_votes\"], errors=\"coerce\")\n",
    "\n",
    "    cond_dep = (\n",
    "        df[\"valid_votes\"].notna() &\n",
    "        df[\"votes\"].notna() &\n",
    "        (df[\"votes\"] < df[\"valid_votes\"] / 6)\n",
    "    )\n",
    "    df[\"deposit_lost\"] = cond_dep.map({True: \"yes\", False: \"no\"})\n",
    "else:\n",
    "    df[\"deposit_lost\"] = \"no\"\n",
    "\n",
    "# =====================================================\n",
    "# 7. MARGIN & MARGIN_PERCENTAGE\n",
    "# =====================================================\n",
    "print(\"Step 8: Calculating Margin and Margin_Percentage...\")\n",
    "\n",
    "# initialise with 0\n",
    "df[\"margin\"] = 0\n",
    "df[\"margin_percentage\"] = 0.0\n",
    "\n",
    "# --- case-insensitive column map ---\n",
    "colmap = {c.lower(): c for c in df.columns}\n",
    "\n",
    "votes_col = colmap.get(\"votes\")\n",
    "const_col = colmap.get(\"constituency_no\")\n",
    "pos_col   = colmap.get(\"position\")\n",
    "vsp_col   = colmap.get(\"vote_share_percentage\")  # adjust here if your name differs\n",
    "\n",
    "# 1) Margin in votes: difference with next candidate by position within each constituency\n",
    "if votes_col and const_col and pos_col:\n",
    "    tmp = df.sort_values([const_col, pos_col])\n",
    "    next_votes = tmp.groupby(const_col)[votes_col].shift(-1)\n",
    "    df[\"__next_votes\"] = next_votes\n",
    "\n",
    "    df[\"margin\"] = (df[votes_col] - df[\"__next_votes\"]).where(df[\"__next_votes\"].notna(), 0)\n",
    "    df[\"margin\"] = df[\"margin\"].astype(int)\n",
    "\n",
    "# 2) Margin in vote share percentage: same logic, using vote_share_percentage\n",
    "if vsp_col and const_col and pos_col:\n",
    "    tmp = df.sort_values([const_col, pos_col])\n",
    "    next_vsp = tmp.groupby(const_col)[vsp_col].shift(-1)\n",
    "    df[\"__next_vsp\"] = next_vsp\n",
    "\n",
    "    df[\"margin_percentage\"] = (\n",
    "        df[vsp_col] - df[\"__next_vsp\"]\n",
    "    ).where(df[\"__next_vsp\"].notna(), 0).round(2)\n",
    "\n",
    "# clean helper columns\n",
    "df.drop(columns=[\"__next_votes\", \"__next_vsp\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# =====================================================\n",
    "# 8. NORMALISE Sex & State_Name\n",
    "# =====================================================\n",
    "print(\"Step 9: Normalising Sex and State_Name...\")\n",
    "\n",
    "if \"sex\" in df.columns:\n",
    "    df[\"sex\"] = df[\"sex\"].astype(str).str.upper().str.strip()\n",
    "    df[\"sex\"] = df[\"sex\"].replace({\n",
    "        \"MALE\": \"M\",\n",
    "        \"FEMALE\": \"F\",\n",
    "        \"THIRD\": \"O\",\n",
    "        \"OTHERS\": \"O\",\n",
    "        \"OTHER\": \"O\"\n",
    "    })\n",
    "    df[\"sex\"] = df[\"sex\"].replace({\"NAN\": \"\", \"NA\": \"\"})\n",
    "\n",
    "if \"state_name\" in df.columns:\n",
    "    df[\"state_name\"] = df[\"state_name\"].astype(str).str.replace(\" \", \"_\").str.upper()\n",
    "\n",
    "# =====================================================\n",
    "# 9. COLUMN ORDER\n",
    "# =====================================================\n",
    "print(\"Step 10: Reordering columns...\")\n",
    "\n",
    "main_cols = [\n",
    "    \"delimid\", \"state_name\", \"assembly_no\", \"year\", \"month\", \"poll_no\", \"election_type\",\n",
    "    \"constituency_no\", \"constituency_name\", \"constituency_type\", \"electors\", \"valid_votes\", \"n_cand\",\n",
    "    \"position\", \"candidate\", \"sex\", \"age\", \"category\", \"party\",\n",
    "    \"votes\", \"vote_share_percentage\", \"electors_share_percentage\",\n",
    "    \"margin\", \"margin_percentage\", \"deposit_lost\"\n",
    "]\n",
    "\n",
    "existing_main = [c for c in main_cols if c in df.columns]\n",
    "other_cols = [c for c in df.columns if c not in existing_main]\n",
    "df = df[existing_main + other_cols]\n",
    "\n",
    "# =====================================================\n",
    "# 10. WRITE OUTPUT\n",
    "# =====================================================\n",
    "print(f\"\\nStep 11: Writing {len(df)} rows to '{CLEANED_ELECTION_CSV}'...\")\n",
    "df.to_csv(CLEANED_ELECTION_CSV, index=False, quoting=csv.QUOTE_ALL)\n",
    "print(\"\\nâœ… Done: metadata, N_Cand, Margin, Margin_Percentage, deposit_lost updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bd135f-ef7f-45cd-9a58-ac7995b070aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE 4 ####\n",
    "# - Compare \"constituency_map_json\" and \"election_csv\" \n",
    "#   on: number_of_rows, page_number_start, page_number_end\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# ======= CONFIGURE YOUR PATHS =======\n",
    "print(\"Loading JSON and CSV...\")\n",
    "\n",
    "# ======= LOAD JSON (CONSTITUENCY_MAP_JSON) =======\n",
    "with open(CONSTITUENCY_MAP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_json = json.load(f)\n",
    "\n",
    "if isinstance(raw_json, list):\n",
    "    json_df = pd.DataFrame(raw_json)\n",
    "elif isinstance(raw_json, dict):\n",
    "    if \"data\" in raw_json and isinstance(raw_json[\"data\"], list):\n",
    "        json_df = pd.DataFrame(raw_json[\"data\"])\n",
    "    else:\n",
    "        json_df = pd.DataFrame(list(raw_json.values()))\n",
    "else:\n",
    "    raise ValueError(\"Unexpected JSON structure\")\n",
    "\n",
    "# Required columns in JSON\n",
    "json_required_cols = [\n",
    "    \"constituency_no\",\n",
    "    \"number_of_rows\",\n",
    "    \"page_number_start\",\n",
    "    \"page_number_end\"\n",
    "]\n",
    "missing_json = [c for c in json_required_cols if c not in json_df.columns]\n",
    "if missing_json:\n",
    "    raise KeyError(f\"JSON is missing required columns: {missing_json}\")\n",
    "\n",
    "# Keep only what we need and normalise types\n",
    "json_df = json_df[json_required_cols].copy()\n",
    "\n",
    "for col in [\"constituency_no\", \"number_of_rows\", \"page_number_start\", \"page_number_end\"]:\n",
    "    json_df[col] = pd.to_numeric(json_df[col], errors=\"coerce\")\n",
    "\n",
    "json_df = json_df.dropna(subset=[\"constituency_no\"])\n",
    "json_df[\"constituency_no\"] = json_df[\"constituency_no\"].astype(int)\n",
    "\n",
    "json_counts = (\n",
    "    json_df\n",
    "    .groupby(\"constituency_no\", as_index=False)\n",
    "    .agg({\n",
    "        \"number_of_rows\": \"max\",          # expected number of rows\n",
    "        \"page_number_start\": \"min\",       # first page where it appears\n",
    "        \"page_number_end\": \"max\"          # last page where it appears\n",
    "    })\n",
    "    .rename(columns={\n",
    "        \"number_of_rows\": \"number_of_rows_json\",\n",
    "        \"page_number_start\": \"page_number_start_json\",\n",
    "        \"page_number_end\": \"page_number_end_json\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# ======= LOAD CSV =======\n",
    "csv_df = pd.read_csv(CLEANED_ELECTION_CSV)\n",
    "\n",
    "# Required columns in CSV\n",
    "csv_required_cols = [\n",
    "    \"constituency_no\",\n",
    "    \"number_of_rows\",\n",
    "    \"page_number_start\",\n",
    "    \"page_number_end\"\n",
    "]\n",
    "missing_csv = [c for c in csv_required_cols if c not in csv_df.columns]\n",
    "if missing_csv:\n",
    "    raise KeyError(f\"CSV is missing required columns: {missing_csv}\")\n",
    "\n",
    "for col in [\"constituency_no\", \"number_of_rows\", \"page_number_start\", \"page_number_end\"]:\n",
    "    csv_df[col] = pd.to_numeric(csv_df[col], errors=\"coerce\")\n",
    "\n",
    "csv_df = csv_df.dropna(subset=[\"constituency_no\"])\n",
    "csv_df[\"constituency_no\"] = csv_df[\"constituency_no\"].astype(int)\n",
    "\n",
    "csv_counts = (\n",
    "    csv_df\n",
    "    .groupby(\"constituency_no\", as_index=False)\n",
    "    .agg({\n",
    "        \"number_of_rows\": \"max\",\n",
    "        \"page_number_start\": \"min\",\n",
    "        \"page_number_end\": \"max\"\n",
    "    })\n",
    "    .rename(columns={\n",
    "        \"number_of_rows\": \"number_of_rows_csv\",\n",
    "        \"page_number_start\": \"page_number_start_csv\",\n",
    "        \"page_number_end\": \"page_number_end_csv\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# ======= MERGE & COMPARE =======\n",
    "merged = json_counts.merge(\n",
    "    csv_counts,\n",
    "    on=\"constituency_no\",\n",
    "    how=\"outer\"\n",
    ").sort_values(\"constituency_no\")\n",
    "\n",
    "# Fill NaNs with 0 for comparison, then cast to int\n",
    "for col in [\n",
    "    \"number_of_rows_json\", \"number_of_rows_csv\",\n",
    "    \"page_number_start_json\", \"page_number_start_csv\",\n",
    "    \"page_number_end_json\", \"page_number_end_csv\"\n",
    "]:\n",
    "    merged[col] = merged[col].fillna(0).astype(int)\n",
    "\n",
    "# Diffs\n",
    "merged[\"diff_rows_csv_minus_json\"] = merged[\"number_of_rows_csv\"] - merged[\"number_of_rows_json\"]\n",
    "merged[\"diff_page_start_csv_minus_json\"] = merged[\"page_number_start_csv\"] - merged[\"page_number_start_json\"]\n",
    "merged[\"diff_page_end_csv_minus_json\"] = merged[\"page_number_end_csv\"] - merged[\"page_number_end_json\"]\n",
    "\n",
    "# Boolean flags for readability\n",
    "merged[\"rows_mismatch\"] = merged[\"number_of_rows_csv\"] != merged[\"number_of_rows_json\"]\n",
    "merged[\"page_start_mismatch\"] = merged[\"page_number_start_csv\"] != merged[\"page_number_start_json\"]\n",
    "merged[\"page_end_mismatch\"] = merged[\"page_number_end_csv\"] != merged[\"page_number_end_json\"]\n",
    "\n",
    "# ======= PRINT ONLY DIFFERENCES =======\n",
    "diff_df = merged[\n",
    "    merged[\"rows_mismatch\"] |\n",
    "    merged[\"page_start_mismatch\"] |\n",
    "    merged[\"page_end_mismatch\"]\n",
    "]\n",
    "\n",
    "if diff_df.empty:\n",
    "    print(\"No differences: all constituency_no have matching number_of_rows and page ranges in JSON and CSV.\")\n",
    "else:\n",
    "    # Optional: choose a column order to make it very clear\n",
    "    cols_order = [\n",
    "        \"constituency_no\",\n",
    "        \"number_of_rows_json\", \"number_of_rows_csv\", \"diff_rows_csv_minus_json\", \"rows_mismatch\",\n",
    "        \"page_number_start_json\", \"page_number_start_csv\", \"diff_page_start_csv_minus_json\", \"page_start_mismatch\",\n",
    "        \"page_number_end_json\", \"page_number_end_csv\", \"diff_page_end_csv_minus_json\", \"page_end_mismatch\"\n",
    "    ]\n",
    "    existing_cols = [c for c in cols_order if c in diff_df.columns]\n",
    "    print(diff_df[existing_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd08d9b-69c4-4117-b8a2-dd9b0bf9d7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
